{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f3e39063",
   "metadata": {},
   "source": [
    "# ADS 509 Module 3: Group Comparison \n",
    "\n",
    "The task of comparing two groups of text is fundamental to textual analysis. There are innumerable applications: survey respondents from different segments of customers, speeches by different political parties, words used in Tweets by different constituencies, etc. In this assignment you will build code to effect comparisons between groups of text data, using the ideas learned in reading and lecture.\n",
    "\n",
    "This assignment asks you to analyze the lyrics for the two artists you selected in Module 1 and the Twitter descriptions pulled for Robyn and Cher. If the results from that pull were not to your liking, you are welcome to use the zipped data from the ‚ÄúAssignment Materials‚Äù section. Specifically, you are asked to do the following: \n",
    "\n",
    "* Read in the data, normalize the text, and tokenize it. When you tokenize your Twitter descriptions, keep hashtags and emojis in your token set. \n",
    "* Calculate descriptive statistics on the two sets of lyrics and compare the results. \n",
    "* For each of the four corpora, find the words that are unique to that corpus. \n",
    "* Build word clouds for all four corpora. \n",
    "\n",
    "Each one of the analyses has a section dedicated to it below. Before beginning the analysis there is a section for you to read in the data and do your cleaning (tokenization and normalization). \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6e65f73",
   "metadata": {},
   "source": [
    "## General Assignment Instructions\n",
    "\n",
    "These instructions are included in every assignment, to remind you of the coding standards for the class. Feel free to delete this cell after reading it. \n",
    "\n",
    "One sign of mature code is conforming to a style guide. We recommend the [Google Python Style Guide](https://google.github.io/styleguide/pyguide.html). If you use a different style guide, please include a cell with a link. \n",
    "\n",
    "Your code should be relatively easy-to-read, sensibly commented, and clean. Writing code is a messy process, so please be sure to edit your final submission. Remove any cells that are not needed or parts of cells that contain unnecessary code. Remove inessential `import` statements and make sure that all such statements are moved into the designated cell. \n",
    "\n",
    "Make use of non-code cells for written commentary. These cells should be grammatical and clearly written. In some of these cells you will have questions to answer. The questions will be marked by a \"Q:\" and will have a corresponding \"A:\" spot for you. *Make sure to answer every question marked with a `Q:` for full credit.* \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "abe420bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import emoji\n",
    "import pandas as pd\n",
    "\n",
    "from collections import Counter, defaultdict\n",
    "from nltk.corpus import stopwords\n",
    "from string import punctuation\n",
    "from wordcloud import WordCloud \n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfTransformer, CountVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "20ba22bf-c729-4155-b9b7-f5f9d818f6d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Speed up membership tests\n",
    "punctuation = set(punctuation)\n",
    "# Twitter-specific punctuation: keep hashtags\n",
    "tw_punct = punctuation - {'#'}\n",
    "# English stopwords\n",
    "sw = set(stopwords.words('english'))\n",
    "# Whitespace splitting regex\n",
    "_whitespace_re = re.compile(r\"\\s+\")\n",
    "# Hashtag pattern\n",
    "_hashtag_re = re.compile(r\"^#[0-9A-Za-z]+\")\n",
    "\n",
    "# ----- Utility Functions -----\n",
    "def contains_emoji(s):\n",
    "    \"\"\"Return True if any character in string is an emoji.\"\"\"\n",
    "    return any(emoji.is_emoji(ch) for ch in str(s))\n",
    "\n",
    "\n",
    "def remove_stop(tokens):\n",
    "    \"\"\"Filter out English stopwords from a list of tokens.\"\"\"\n",
    "    return [t for t in tokens if t.lower() not in sw]\n",
    "\n",
    "\n",
    "def remove_punctuation(text, punct_set=tw_punct):\n",
    "    \"\"\"Remove punctuation characters from text.\"\"\"\n",
    "    return ''.join(ch for ch in text if ch not in punct_set)\n",
    "\n",
    "\n",
    "def tokenize(text):\n",
    "    \"\"\"Split text on whitespace (preserve hashtags & emojis).\"\"\"\n",
    "    return [tok for tok in _whitespace_re.split(str(text)) if tok]\n",
    "\n",
    "\n",
    "def prepare(text, pipeline):\n",
    "    \"\"\"Apply a sequence of transform functions to raw text.\"\"\"\n",
    "    data = text\n",
    "    for fn in pipeline:\n",
    "        data = fn(data)\n",
    "    return data\n",
    "\n",
    "\n",
    "def descriptive_stats(tokens, num_tokens=5, verbose=True):\n",
    "    \"\"\"\n",
    "    Given a list of tokens, compute and optionally print:\n",
    "      - total number of tokens\n",
    "      - number of unique tokens\n",
    "      - number of characters across tokens\n",
    "      - lexical diversity = unique/total\n",
    "      - top `num_tokens` most common tokens\n",
    "    Returns [total, unique, diversity, characters].\n",
    "    \"\"\"\n",
    "    total = len(tokens)\n",
    "    unique = len(set(tokens))\n",
    "    chars = sum(len(t) for t in tokens)\n",
    "    diversity = unique / total if total else 0.0\n",
    "\n",
    "    if verbose:\n",
    "        print(f\"There are {total} tokens in the data.\")\n",
    "        print(f\"There are {unique} unique tokens in the data.\")\n",
    "        print(f\"There are {chars} characters in the data.\")\n",
    "        print(f\"The lexical diversity is {diversity:.3f} in the data.\")\n",
    "        print(f\"\\nThe {num_tokens} most common tokens are:\")\n",
    "        for tok, cnt in Counter(tokens).most_common(num_tokens):\n",
    "            print(f\"  {tok}: {cnt}\")\n",
    "\n",
    "    return [total, unique, diversity, chars]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47735524",
   "metadata": {},
   "source": [
    "## Data Ingestion\n",
    "\n",
    "Use this section to ingest your data into the data structures you plan to use. Typically this will be a dictionary or a pandas DataFrame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "ce3d0ce3-c32d-4d03-a113-d0b642b3d9e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_location = \"/Users/bobbymarriott/Downloads/M1 Results\"\n",
    "twitter_folder = \"twitter\"\n",
    "lyrics_folder  = \"lyrics\"\n",
    "\n",
    "artist_twitter_files = {\n",
    "    \"cher\": \"cher_followers_data.txt\",\n",
    "    \"robyn\": \"robynkonichiwa_followers_data.txt\"\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "df415d65",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded Twitter: (4353175, 8)\n",
      "artist\n",
      "cher     3994803\n",
      "robyn     358372\n",
      "Name: count, dtype: int64 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "twitter_frames = []\n",
    "for artist, fname in artist_twitter_files.items():\n",
    "    path = os.path.join(data_location, twitter_folder, fname)\n",
    "    df   = pd.read_csv(path, sep=\"\\t\", quoting=3, encoding=\"utf-8\")\n",
    "    df[\"artist\"] = artist\n",
    "    twitter_frames.append(df)\n",
    "\n",
    "twitter_data = pd.concat(twitter_frames, ignore_index=True)\n",
    "del twitter_frames  # tidy up\n",
    "\n",
    "print(\"Loaded Twitter:\", twitter_data.shape)\n",
    "print(twitter_data.artist.value_counts(), \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "966804cb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 316 lyrics for cher\n",
      "Loaded 104 lyrics for robyn\n"
     ]
    }
   ],
   "source": [
    "lyrics_data = {}\n",
    "for artist in artist_twitter_files:\n",
    "    artist_dir = os.path.join(data_location, lyrics_folder, artist)\n",
    "    texts = []\n",
    "    for fn in sorted(os.listdir(artist_dir)):\n",
    "        if not fn.endswith(\".txt\"):\n",
    "            continue\n",
    "        with open(os.path.join(artist_dir, fn), encoding=\"utf-8\") as f:\n",
    "            texts.append(f.read())\n",
    "    lyrics_data[artist] = texts\n",
    "    print(f\"Loaded {len(texts)} lyrics for {artist}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9892d14",
   "metadata": {},
   "source": [
    "## Tokenization and Normalization\n",
    "\n",
    "In this next section, tokenize and normalize your data. We recommend the following cleaning. \n",
    "\n",
    "**Lyrics** \n",
    "\n",
    "* Remove song titles\n",
    "* Casefold to lowercase\n",
    "* Remove stopwords (optional)\n",
    "* Remove punctuation\n",
    "* Split on whitespace\n",
    "\n",
    "Removal of stopwords is up to you. Your descriptive statistic comparison will be different if you include stopwords, though TF-IDF should still find interesting features for you. Note that we remove stopwords before removing punctuation because the stopword set includes punctuation.\n",
    "\n",
    "**Twitter Descriptions** \n",
    "\n",
    "* Casefold to lowercase\n",
    "* Remove stopwords\n",
    "* Remove punctuation other than emojis or hashtags\n",
    "* Split on whitespace\n",
    "\n",
    "Removing stopwords seems sensible for the Twitter description data. Remember to leave in emojis and hashtags, since you analyze those. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "3949e7f9-3ebb-4f90-b34d-872ad140b10f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Lyrics sample ===\n",
      "  artist  num_tokens\n",
      "0  robyn         205\n",
      "1  robyn          66\n",
      "2  robyn         119\n",
      "3  robyn          77\n",
      "4  robyn         174\n",
      "\n",
      "=== Twitter sample ===\n",
      "  artist  num_tokens\n",
      "0   cher           0\n",
      "1   cher           6\n",
      "2   cher           3\n",
      "3   cher           1\n",
      "4   cher          17\n"
     ]
    }
   ],
   "source": [
    "rows = []\n",
    "lyrics_base = os.path.join(data_location, lyrics_folder)\n",
    "for artist in os.listdir(lyrics_base):\n",
    "    artist_dir = os.path.join(lyrics_base, artist)\n",
    "    if not os.path.isdir(artist_dir):\n",
    "        continue\n",
    "\n",
    "    for fn in sorted(os.listdir(artist_dir)):\n",
    "        if not fn.endswith(\".txt\"):\n",
    "            continue\n",
    "        path = os.path.join(artist_dir, fn)\n",
    "        with open(path, \"r\", encoding=\"utf-8\") as f:\n",
    "            text = f.read()\n",
    "        rows.append({\n",
    "            \"artist\": artist,\n",
    "            \"lyrics\": text\n",
    "        })\n",
    "\n",
    "lyrics_data = pd.DataFrame(rows)\n",
    "\n",
    "\n",
    "lyrics_data[\"lyrics\"]      = lyrics_data[\"lyrics\"]     .fillna(\"\").astype(str)\n",
    "twitter_data[\"description\"] = twitter_data[\"description\"].fillna(\"\").astype(str)\n",
    "\n",
    "\n",
    "# make prepare safe on non-strings\n",
    "def prepare(text, pipeline):\n",
    "    text = text if isinstance(text, str) else \"\"\n",
    "    for fn in pipeline:\n",
    "        text = fn(text)\n",
    "    return text\n",
    "\n",
    "\n",
    "# rebuild pipeline\n",
    "my_pipeline = [\n",
    "    str.lower,        # case‚Äêfold\n",
    "    remove_punctuation,  # drop punctuation\n",
    "    tokenize,         # split on whitespace\n",
    "    remove_stop       # drop stopwords\n",
    "]\n",
    "\n",
    "# apply to lyrics\n",
    "lyrics_data[\"tokens\"]     = lyrics_data[\"lyrics\"]     .apply(lambda x: prepare(x, my_pipeline))\n",
    "lyrics_data[\"num_tokens\"] = lyrics_data[\"tokens\"]     .map(len)\n",
    "\n",
    "# apply to twitter descriptions\n",
    "twitter_data[\"tokens\"]     = twitter_data[\"description\"].apply(lambda x: prepare(x, my_pipeline))\n",
    "twitter_data[\"num_tokens\"] = twitter_data[\"tokens\"].map(len)\n",
    "\n",
    "print(\"=== Lyrics sample ===\")\n",
    "print(lyrics_data[[\"artist\",\"num_tokens\"]].head())\n",
    "print(\"\\n=== Twitter sample ===\")\n",
    "print(twitter_data[[\"artist\",\"num_tokens\"]].head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "6cf534be",
   "metadata": {},
   "outputs": [],
   "source": [
    "twitter_data['has_emoji'] = twitter_data[\"description\"].apply(contains_emoji)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ec69ac9",
   "metadata": {},
   "source": [
    "Let's take a quick look at some descriptions with emojis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "0a5a0512",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>artist</th>\n",
       "      <th>description</th>\n",
       "      <th>tokens</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2397996</th>\n",
       "      <td>cher</td>\n",
       "      <td>I Love Justin Bieber ‚ô•‚ô°‚ô• But His Music Tho ‚ô•‚ô¨‚ô™‚ô•</td>\n",
       "      <td>[love, justin, bieber, ‚ô•‚ô°‚ô•, music, tho, ‚ô•‚ô¨‚ô™‚ô•]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>684664</th>\n",
       "      <td>cher</td>\n",
       "      <td>aqui vemos uma pessoa que n√£o sabe oq est√° faz...</td>\n",
       "      <td>[aqui, vemos, uma, pessoa, que, n√£o, sabe, oq,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>920091</th>\n",
       "      <td>cher</td>\n",
       "      <td>My favs, sometimes 18+, men, porn, funny, horn...</td>\n",
       "      <td>[favs, sometimes, 18, men, porn, funny, hornyüòÅ‚ù§Ô∏è]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1637272</th>\n",
       "      <td>cher</td>\n",
       "      <td>Wrestling Fan/xbox gammer.Hopeless Romantic. P...</td>\n",
       "      <td>[wrestling, fanxbox, gammerhopeless, romantic,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3383606</th>\n",
       "      <td>cher</td>\n",
       "      <td>BE IN CONTROL OF YOUR DESTINY&amp;ALWAYS KEEP YOUR...</td>\n",
       "      <td>[control, destinyalways, keep, eyes, prize, ¬•,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3934097</th>\n",
       "      <td>cher</td>\n",
       "      <td>Tomorrow never knows what it doesn‚Äôt know too ...</td>\n",
       "      <td>[tomorrow, never, knows, doesn‚Äôt, know, soon, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3345938</th>\n",
       "      <td>cher</td>\n",
       "      <td>Be yourself to free yourself üåà</td>\n",
       "      <td>[free, üåà]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>925793</th>\n",
       "      <td>cher</td>\n",
       "      <td>artist writer movie fanatic scifi fantasy geek...</td>\n",
       "      <td>[artist, writer, movie, fanatic, scifi, fantas...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1002334</th>\n",
       "      <td>cher</td>\n",
       "      <td>| voluminous hair | ole miss | I ‚ù§Ô∏è bears | ma...</td>\n",
       "      <td>[voluminous, hair, ole, miss, ‚ù§Ô∏è, bears, may, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>419115</th>\n",
       "      <td>cher</td>\n",
       "      <td>love yourselfüòá BeYoutifulüíû Learn to Forgive yo...</td>\n",
       "      <td>[love, yourselfüòá, beyoutifulüíû, learn, forgive,...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        artist                                        description  \\\n",
       "2397996   cher    I Love Justin Bieber ‚ô•‚ô°‚ô• But His Music Tho ‚ô•‚ô¨‚ô™‚ô•   \n",
       "684664    cher  aqui vemos uma pessoa que n√£o sabe oq est√° faz...   \n",
       "920091    cher  My favs, sometimes 18+, men, porn, funny, horn...   \n",
       "1637272   cher  Wrestling Fan/xbox gammer.Hopeless Romantic. P...   \n",
       "3383606   cher  BE IN CONTROL OF YOUR DESTINY&ALWAYS KEEP YOUR...   \n",
       "3934097   cher  Tomorrow never knows what it doesn‚Äôt know too ...   \n",
       "3345938   cher                     Be yourself to free yourself üåà   \n",
       "925793    cher  artist writer movie fanatic scifi fantasy geek...   \n",
       "1002334   cher  | voluminous hair | ole miss | I ‚ù§Ô∏è bears | ma...   \n",
       "419115    cher  love yourselfüòá BeYoutifulüíû Learn to Forgive yo...   \n",
       "\n",
       "                                                    tokens  \n",
       "2397996      [love, justin, bieber, ‚ô•‚ô°‚ô•, music, tho, ‚ô•‚ô¨‚ô™‚ô•]  \n",
       "684664   [aqui, vemos, uma, pessoa, que, n√£o, sabe, oq,...  \n",
       "920091   [favs, sometimes, 18, men, porn, funny, hornyüòÅ‚ù§Ô∏è]  \n",
       "1637272  [wrestling, fanxbox, gammerhopeless, romantic,...  \n",
       "3383606  [control, destinyalways, keep, eyes, prize, ¬•,...  \n",
       "3934097  [tomorrow, never, knows, doesn‚Äôt, know, soon, ...  \n",
       "3345938                                          [free, üåà]  \n",
       "925793   [artist, writer, movie, fanatic, scifi, fantas...  \n",
       "1002334  [voluminous, hair, ole, miss, ‚ù§Ô∏è, bears, may, ...  \n",
       "419115   [love, yourselfüòá, beyoutifulüíû, learn, forgive,...  "
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "twitter_data[twitter_data.has_emoji].sample(10)[[\"artist\",\"description\",\"tokens\"]]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b2c55c9",
   "metadata": {},
   "source": [
    "With the data processed, we can now start work on the assignment questions. \n",
    "\n",
    "Q: What is one area of improvement to your tokenization that you could theoretically carry out? (No need to actually do it; let's not make perfect the enemy of good enough.)\n",
    "\n",
    "A: Switching from a na√Øve whitespace split to NLTK's TweetTokenizer may help. This can help with commas, periods, etc. as certain emojis/hashtags may be recognized as a single token instead of lumped. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1594271",
   "metadata": {},
   "source": [
    "## Calculate descriptive statistics on the two sets of lyrics and compare the results. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "dc25e937",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Cher lyrics ===\n",
      "There are 35916 tokens in the data.\n",
      "There are 3703 unique tokens in the data.\n",
      "There are 172634 characters in the data.\n",
      "The lexical diversity is 0.103 in the data.\n",
      "\n",
      "The 5 most common tokens are:\n",
      "  love: 1004\n",
      "  im: 513\n",
      "  know: 486\n",
      "  dont: 440\n",
      "  youre: 333\n",
      "\n",
      "=== Robyn lyrics ===\n",
      "There are 15227 tokens in the data.\n",
      "There are 2156 unique tokens in the data.\n",
      "There are 73787 characters in the data.\n",
      "The lexical diversity is 0.142 in the data.\n",
      "\n",
      "The 5 most common tokens are:\n",
      "  know: 308\n",
      "  dont: 301\n",
      "  im: 299\n",
      "  love: 275\n",
      "  got: 251\n"
     ]
    }
   ],
   "source": [
    "cher_tokens = [tok\n",
    "               for tokens in lyrics_data.loc[lyrics_data['artist']=='cher','tokens']\n",
    "               for tok in tokens]\n",
    "robyn_tokens = [tok\n",
    "                for tokens in lyrics_data.loc[lyrics_data['artist']=='robyn','tokens']\n",
    "                for tok in tokens]\n",
    "\n",
    "print(\"=== Cher lyrics ===\")\n",
    "cher_stats = descriptive_stats(cher_tokens, verbose=True)\n",
    "\n",
    "print(\"\\n=== Robyn lyrics ===\")\n",
    "robyn_stats = descriptive_stats(robyn_tokens, verbose=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67a2ada9",
   "metadata": {},
   "source": [
    "Q: what observations do you make about these data? \n",
    "\n",
    "A: Cher's lyric amounts to over 35,000 tokens but has pretty low diversity (0.103). On the other hand, Robyn's 15,000 tokens has a higher diversity (0.142), which indicates that this artist has a larger range of words/diction. Additionally, Cher has used the word 'love' the most while Robyn leads with 'know', which shows a different type of tone in song. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "750aa526",
   "metadata": {},
   "source": [
    "## Find tokens uniquely related to a corpus\n",
    "\n",
    "Typically we would use TF-IDF to find unique tokens in documents. Unfortunately, we either have too few documents (if we view each data source as a single document) or too many (if we view each description as a separate document). In the latter case, our problem will be that descriptions tend to be short, so our matrix would be too sparse to support analysis. \n",
    "\n",
    "To avoid these problems, we will create a custom statistic to identify words that are uniquely related to each corpus. The idea is to find words that occur often in one corpus and infrequently in the other(s). Since corpora can be of different lengths, we will focus on the _concentration_ of tokens within a corpus. \"Concentration\" is simply the count of the token divided by the total corpus length. For instance, if a corpus had length 100,000 and a word appeared 1,000 times, then the concentration would be $\\frac{1000}{100000} = 0.01$. If the same token had a concentration of $0.005$ in another corpus, then the concentration ratio would be $\\frac{0.01}{0.005} = 2$. Very rare words can easily create infinite ratios, so you will also add a cutoff to your code so that a token must appear at least $n$ times for you to return it. \n",
    "\n",
    "An example of these calculations can be found in [this spreadsheet](https://docs.google.com/spreadsheets/d/1P87fkyslJhqXFnfYezNYrDrXp_GS8gwSATsZymv-9ms). Please don't hesitate to ask questions if this is confusing. \n",
    "\n",
    "In this section find 10 tokens for each of your four corpora that meet the following criteria: \n",
    "\n",
    "1. The token appears at least `n` times in all corpora\n",
    "1. The tokens are in the top 10 for the highest ratio of appearances in a given corpora vs appearances in other corpora.\n",
    "\n",
    "You will choose a cutoff for yourself based on the side of the corpus you're working with. If you're working with the Robyn-Cher corpora provided, `n=5` seems to perform reasonably well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "ce72f0a0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Top 10 unique tokens for cher lyrics ===\n",
      "['alegr√£\\xada', 'wontcha', 'geronimos', 'nooh', 'woahoh', 'milord', 'repossessing', 'rhymney', 'guilded', 'chiquitita']\n",
      "\n",
      "=== Top 10 unique tokens for cher twitter ===\n",
      "['csu', '#dcnative', 'sexagenarian', 'saunas', 'romanos', '831', 'Íï•', 'masseuse', '#pagan', 'antibigot']\n",
      "\n",
      "=== Top 10 unique tokens for robyn lyrics ===\n",
      "['moneyman', 'tjaffs', 'bububurn', 'c√¢\\x80\\x99mon', 'headlessly', 'ultramagnetic', 'apr√£¬©ndelo', 'yyou', 'rudegirl', 'transistors']\n",
      "\n",
      "=== Top 10 unique tokens for robyn twitter ===\n",
      "['cykla', 'd√§remellan', 'musikproducent', 'framf√∂rallt', '√∏konomi', 'n√¶sten', 'promenader', 'st√§lle', 'j√§mf√∂r', 'bl√•vitt']\n"
     ]
    }
   ],
   "source": [
    "corpora_tokens = {}\n",
    "for artist in [\"cher\", \"robyn\"]:\n",
    "    # lyrics\n",
    "    lyr = lyrics_data.loc[lyrics_data[\"artist\"] == artist, \"tokens\"]\n",
    "    corpora_tokens[f\"{artist}_lyrics\"] = [tok for row in lyr for tok in row]\n",
    "    # twitter\n",
    "    tw = twitter_data.loc[twitter_data[\"artist\"] == artist, \"tokens\"]\n",
    "    corpora_tokens[f\"{artist}_twitter\"] = [tok for row in tw for tok in row]\n",
    "\n",
    "# 2. counts & totals\n",
    "counts = {name: Counter(toks) for name, toks in corpora_tokens.items()}\n",
    "totals = {name: len(toks)          for name, toks in corpora_tokens.items()}\n",
    "global_counts = Counter(tok for toks in corpora_tokens.values() for tok in toks)\n",
    "grand_total   = sum(totals.values())\n",
    "\n",
    "# 3. compute concentration-ratio and pick top 10 per corpus\n",
    "n = 5  # minimum count in target corpus\n",
    "unique_tokens = {}\n",
    "for name, ctr in counts.items():\n",
    "    other_total = grand_total - totals[name]\n",
    "    other_counts = global_counts - ctr\n",
    "\n",
    "    scores = {}\n",
    "    for tok, cnt in ctr.items():\n",
    "        if cnt < n:\n",
    "            continue\n",
    "        conc = cnt / totals[name]\n",
    "        conc_other = other_counts[tok] / other_total if other_total > 0 else 0.0\n",
    "        # if token never appears elsewhere, give it an \"infinite\" score\n",
    "        scores[tok] = conc / conc_other if conc_other > 0 else float(\"inf\")\n",
    "\n",
    "    # top 10 by descending score\n",
    "    top10 = sorted(scores.items(), key=lambda x: x[1], reverse=True)[:10]\n",
    "    unique_tokens[name] = [tok for tok, score in top10]\n",
    "\n",
    "for name, toks in unique_tokens.items():\n",
    "    print(f\"\\n=== Top 10 unique tokens for {name.replace('_',' ')} ===\")\n",
    "    print(toks)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53526fcd",
   "metadata": {},
   "source": [
    "Q: What are some observations about the top tokens? Do you notice any interesting items on the list? \n",
    "\n",
    "A: Cher's lyrics tokens have one-off song words such as \"respossessing\" and \"geronimos\" while her Twitter has niche tags. On the other hand, Robyn's lyrics have terms like \"ultarmagnetic\" while her Twitter is completely in a foreign language. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f4f52b3",
   "metadata": {},
   "source": [
    "## Build word clouds for all four corpora. \n",
    "\n",
    "For building wordclouds, we'll follow exactly the code of the text. The code in this section can be found [here](https://github.com/blueprints-for-text-analytics-python/blueprints-text/blob/master/ch01/First_Insights.ipynb). If you haven't already, you should absolutely clone the repository that accompanies the book. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "228b3424-7b99-431c-8179-5056ec927cb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from wordcloud import WordCloud\n",
    "from matplotlib import pyplot as plt\n",
    "from collections import Counter\n",
    "\n",
    "def plot_wc(freq_df, title=None, max_words=200, stopwords=None):\n",
    "    \"\"\"Generate a word-cloud from the freq_df produced by count_words().\"\"\"\n",
    "    # pull out the dict of token‚Üífreq\n",
    "    freqs = freq_df['freq'].to_dict()\n",
    "    # apply stopword filter if requested\n",
    "    if stopwords is not None:\n",
    "        freqs = {tok:cnt for tok,cnt in freqs.items() if tok not in stopwords}\n",
    "    if not freqs:\n",
    "        print(f\"‚ö†Ô∏è  no terms ‚â• cutoff for ‚Äú{title}‚Äù, skipping cloud.\")\n",
    "        return\n",
    "\n",
    "    wc = WordCloud(\n",
    "        width=800, height=400,\n",
    "        background_color='white',\n",
    "        max_words=max_words\n",
    "    ).generate_from_frequencies(freqs)\n",
    "\n",
    "    plt.figure(figsize=(10, 5))\n",
    "    if title:\n",
    "        plt.title(title, fontsize=16)\n",
    "    plt.imshow(wc, interpolation='bilinear')\n",
    "    plt.axis('off')\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "def count_words(df, column='tokens', preprocess=None, min_freq=2):\n",
    "    \"\"\"\n",
    "    Count up all tokens in df[column], optionally piping each doc through preprocess(),\n",
    "    then return a DataFrame of token‚Üífreq for freq>=min_freq.\n",
    "    \"\"\"\n",
    "    counter = Counter()\n",
    "    for doc in df[column]:\n",
    "        toks = preprocess(doc) if preprocess is not None else doc\n",
    "        counter.update(toks)\n",
    "\n",
    "    freq_df = pd.DataFrame.from_dict(\n",
    "        counter, orient='index', columns=['freq']\n",
    "    )\n",
    "    freq_df.index.name = 'token'\n",
    "    # filter on the minimum frequency\n",
    "    freq_df = freq_df.query('freq >= @min_freq') \\\n",
    "                     .sort_values('freq', ascending=False)\n",
    "    return freq_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "685f1be5-4753-40d4-942c-e418c43a5e5c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cher lyrics freq table:\n",
      " Empty DataFrame\n",
      "Columns: [freq]\n",
      "Index: []\n",
      "Robyn lyrics freq table:\n",
      " Empty DataFrame\n",
      "Columns: [freq]\n",
      "Index: []\n",
      "‚ö†Ô∏è  no terms ‚â• cutoff for ‚ÄúCher ‚Äì Lyrics‚Äù, skipping cloud.\n",
      "‚ö†Ô∏è  no terms ‚â• cutoff for ‚ÄúRobyn ‚Äì Lyrics‚Äù, skipping cloud.\n",
      "‚ö†Ô∏è  no terms ‚â• cutoff for ‚ÄúCher ‚Äì Twitter‚Äù, skipping cloud.\n",
      "‚ö†Ô∏è  no terms ‚â• cutoff for ‚ÄúRobyn ‚Äì Twitter‚Äù, skipping cloud.\n"
     ]
    }
   ],
   "source": [
    "# build frequency tables (lyrics at min_freq=5, twitter at min_freq=1)\n",
    "freq_cl = count_words(\n",
    "    lyrics_data[lyrics_data.artist == \"Cher\"],\n",
    "    column=\"tokens\",\n",
    "    min_freq=5\n",
    ")\n",
    "freq_rl = count_words(\n",
    "    lyrics_data[lyrics_data.artist == \"Robyn\"],\n",
    "    column=\"tokens\",\n",
    "    min_freq=5\n",
    ")\n",
    "freq_ct = count_words(\n",
    "    twitter_data[twitter_data.artist == \"Cher\"],\n",
    "    column=\"tokens\",\n",
    "    min_freq=1\n",
    ")\n",
    "freq_rt = count_words(\n",
    "    twitter_data[twitter_data.artist == \"Robyn\"],\n",
    "    column=\"tokens\",\n",
    "    min_freq=1\n",
    ")\n",
    "\n",
    "print(\"Cher lyrics freq table:\\n\", freq_cl.head())\n",
    "print(\"Robyn lyrics freq table:\\n\", freq_rl.head())\n",
    "\n",
    "# plot\n",
    "plot_wc(freq_cl, title=\"Cher ‚Äì Lyrics\",   stopwords=sw)\n",
    "plot_wc(freq_rl, title=\"Robyn ‚Äì Lyrics\", stopwords=sw)\n",
    "plot_wc(freq_ct, title=\"Cher ‚Äì Twitter\", stopwords=sw)\n",
    "plot_wc(freq_rt, title=\"Robyn ‚Äì Twitter\",stopwords=sw)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b11a2e53",
   "metadata": {},
   "source": [
    "Q: What observations do you have about these (relatively straightforward) wordclouds? \n",
    "\n",
    "A: Could not figure this part of assignment out in time prior to submission**"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
